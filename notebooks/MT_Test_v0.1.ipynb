{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zf7jL0azaczQ"
   },
   "source": [
    "# Student Feedback Analysis using Llama 3.1 8B\n",
    "\n",
    "This notebook analyzes student feedback using the Llama 3.1 8B model on Google Colab with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1742396539331,
     "user": {
      "displayName": "Thies Jinadu",
      "userId": "13764533932779643659"
     },
     "user_tz": -60
    },
    "id": "1vkZbyj2djxE",
    "outputId": "d627090e-d9bc-4460-c631-78afa065e268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 19 15:02:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70797,
     "status": "ok",
     "timestamp": 1742396610152,
     "user": {
      "displayName": "Thies Jinadu",
      "userId": "13764533932779643659"
     },
     "user_tz": -60
    },
    "id": "tFGXUdAdjKFX",
    "outputId": "fc7f9a11-4ef6-425e-8e03-930afa91f497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy transformers torch accelerate tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16129,
     "status": "ok",
     "timestamp": 1742396788604,
     "user": {
      "displayName": "Thies Jinadu",
      "userId": "13764533932779643659"
     },
     "user_tz": -60
    },
    "id": "u0mAw6sljlNM",
    "outputId": "da4b7a71-c34b-412e-a94b-1f8e094ace97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1742396849218,
     "user": {
      "displayName": "Thies Jinadu",
      "userId": "13764533932779643659"
     },
     "user_tz": -60
    },
    "id": "BT76F6ihjosN"
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"Load the Llama 3 model and tokenizer\"\"\"\n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            token=os.getenv('HUGGINGFACE_TOKEN')\n",
    "        )\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            token=os.getenv('HUGGINGFACE_TOKEN'),\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1742397984842,
     "user": {
      "displayName": "Thies Jinadu",
      "userId": "13764533932779643659"
     },
     "user_tz": -60
    },
    "id": "r3enUIKhkBQy"
   },
   "outputs": [],
   "source": [
    "def create_master_table(data_dir):\n",
    "    \"\"\"Create a master table aggregating feedback for each student\"\"\"\n",
    "    master_data = []\n",
    "\n",
    "    # Process each CSV file\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                # Read CSV with proper quoting to handle commas in feedback\n",
    "                df = pd.read_csv(\n",
    "                    file_path,\n",
    "                    quoting=csv.QUOTE_ALL,  # Quote all fields\n",
    "                    escapechar='\\\\',        # Use backslash as escape character\n",
    "                    doublequote=True        # Allow double quotes within quoted strings\n",
    "                )\n",
    "\n",
    "                # Verify required columns\n",
    "                required_columns = ['team_number', 'rater_student_id', 'rated_student_id', 'criterion', 'feedback']\n",
    "                if not all(col in df.columns for col in required_columns):\n",
    "                    print(f\"Warning: {filename} missing required columns\")\n",
    "                    continue\n",
    "\n",
    "                # Add to master data\n",
    "                for _, row in df.iterrows():\n",
    "                    master_data.append({\n",
    "                        'Studentnummer': row['rated_student_id'],\n",
    "                        'Criterium': row['criterion'],\n",
    "                        'Feedback': str(row['feedback']).strip(),  # Ensure feedback is string and remove extra whitespace\n",
    "                        'Source': filename,\n",
    "                        'Team': row['team_number'],\n",
    "                        'Rater': row['rater_student_id']\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    if not master_data:\n",
    "        print(\"Warning: No data was processed from any CSV files\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Create master DataFrame\n",
    "    master_df = pd.DataFrame(master_data)\n",
    "\n",
    "    # Group by student and criterion\n",
    "    grouped = master_df.groupby(['Studentnummer', 'Criterium']).agg({\n",
    "        'Feedback': lambda x: '\\n'.join(str(s).strip() for s in x if pd.notna(s)),  # Handle NaN values\n",
    "        'Source': lambda x: ', '.join(x),\n",
    "        'Team': 'first',\n",
    "        'Rater': lambda x: ', '.join(map(str, x))\n",
    "    }).reset_index()\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742397991944,
     "user": {
      "displayName": "Thies Jinadu",
      "userId": "13764533932779643659"
     },
     "user_tz": -60
    },
    "id": "-ka6GFWSkE5M"
   },
   "outputs": [],
   "source": [
    "def analyze_student_feedback(model, tokenizer, feedback_text, criterion):\n",
    "    \"\"\"Analyze feedback text using the model\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Je bent een ervaren onderwijsassistent aan de Katholieke Universiteit Leuven. Je taak is om de volgende feedback van medestudenten te analyseren en samen te vatten voor een individuele student na een collaboratieve probleemoplossende sessie:\n",
    "\n",
    "{feedback_text}\n",
    "\n",
    "# Focus op de volgende context:{criterion}\n",
    "De student heeft deelgenomen aan een groepsopdracht waarbij medestudenten feedback hebben gegeven op drie specifieke criteria:\n",
    "1. Inhoudelijke inbreng - De kwaliteit en relevantie van de bijdragen aan de discussie en het project\n",
    "2. Werkkracht - De inzet, productiviteit en discipline bij het uitvoeren van taken\n",
    "3. Teamspeler - De samenwerking, communicatie en ondersteuning van teamleden\n",
    "\n",
    "# Opdracht\n",
    "Analyseer de volgende feedback en creëer een gestructureerde samenvatting die de student helpt om te groeien volgens het ICAP-model (Interactive, Constructive, Active, Passive). Focus hierbij op:\n",
    "\n",
    "- Hoe de student momenteel presteert binnen elk criterium\n",
    "- Concrete voorbeelden uit de feedback die de sterke punten illustreren\n",
    "- Ontwikkelingsgebieden waar verbetering mogelijk is\n",
    "- Een duidelijk groeipad van passieve naar interactieve betrokkenheid\n",
    "\n",
    "# ICAP-model uitleg\n",
    "Het ICAP-model categoriseert cognitieve betrokkenheid in vier niveaus met toenemende leereffectiviteit:\n",
    "- Passief: Informatie ontvangen zonder actieve verwerking\n",
    "- Actief: Manipuleren van informatie zonder nieuwe ideeën te genereren\n",
    "- Constructief: Genereren van nieuwe ideeën die verder gaan dan de gepresenteerde informatie\n",
    "- Interactief: Dialogeren met anderen en voortbouwen op elkaars bijdragen\n",
    "\n",
    "# Feedback data\n",
    "[Voeg hier alle feedback van medestudenten toe]\n",
    "\n",
    "# Output formaat\n",
    "1. Begin met een korte introductie die de algemene indruk weergeeft\n",
    "2. Analyseer per criterium (Inhoudelijke inbreng, Werkkracht, Teamspeler):\n",
    "   - Huidige niveau volgens ICAP\n",
    "   - Sterke punten met concrete voorbeelden\n",
    "   - Verbeterpunten met suggesties\n",
    "3. Geef een geïntegreerd groeipad dat de student helpt om naar een hoger ICAP-niveau te bewegen\n",
    "4. Sluit af met een bemoedigende samenvatting\n",
    "\n",
    "Schrijf deze samenvatting in een professionele maar toegankelijke academische stijl, zoals een docent aan de KU Leuven zou communiceren. Gebruik specifieke voorbeelden uit de feedback om je punten te illustreren.\"\"\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=2048,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        analysis = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        print(f\"Error in analysis: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1742398061901,
     "user": {
      "displayName": "Thies Jinadu",
      "userId": "13764533932779643659"
     },
     "user_tz": -60
    },
    "id": "qCBx013akKCQ"
   },
   "outputs": [],
   "source": [
    "def process_master_table(model, tokenizer, master_df, output_dir):\n",
    "    \"\"\"Process the master table and generate analyses\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each student-criterion combination\n",
    "    for _, row in tqdm(master_df.iterrows(), total=len(master_df)):\n",
    "        student_id = row['Studentnummer']\n",
    "        criterion = row['Criterium']\n",
    "        feedback = row['Feedback']\n",
    "\n",
    "        # Generate analysis\n",
    "        analysis = analyze_student_feedback(model, tokenizer, feedback, criterion)\n",
    "\n",
    "        if analysis:\n",
    "            results.append({\n",
    "                'Studentnummer': student_id,\n",
    "                'Criterium': criterion,\n",
    "                'Feedback': feedback,\n",
    "                'Analyse': analysis\n",
    "            })\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = os.path.join(output_dir, f'feedback_analysis_{timestamp}.csv')\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212,
     "referenced_widgets": [
      "30c3d1a447524554a66a3a2126617242",
      "ae8f62824c3d4f8eac635485b3532284",
      "b3ac3b381a7e4144bc778d05269c9284",
      "a8cae0c45b1249028946d8028160eae5",
      "20a427473aa54832b14f1d964f2d7deb",
      "1affa6f452934e78987d0266554b952c",
      "86a5def4b609496a8e57518674d0181b",
      "9fde096508bb4e2cb0a257d39b6d5574",
      "56c17f16fbf94c0d845d931285c4fd81",
      "e85b63ef607249faa67da2e2691455a8",
      "776b9735bfb141628f50295c5970d13b"
     ]
    },
    "id": "4BARTLmlkWGn",
    "outputId": "8cc13389-3532-410a-83a5-4930883e2a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c3d1a447524554a66a3a2126617242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating master table...\n",
      "Error processing comments-Table 1.csv: Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n",
      "\n",
      "Master table created with 36 entries\n",
      "Processing master table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/36 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Set Hugging Face token\n",
    "os.environ['HUGGINGFACE_TOKEN'] = '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'\n",
    "\n",
    "# Define directories\n",
    "data_dir = '/content/data'\n",
    "output_dir = '/content/output'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_model()\n",
    "if model is None or tokenizer is None:\n",
    "    raise Exception(\"Failed to load model\")\n",
    "\n",
    "# Create master table\n",
    "print(\"Creating master table...\")\n",
    "master_df = create_master_table(data_dir)\n",
    "print(f\"Master table created with {len(master_df)} entries\")\n",
    "\n",
    "# Process master table\n",
    "print(\"Processing master table...\")\n",
    "results_df = process_master_table(model, tokenizer, master_df, output_dir)\n",
    "print(\"Processing complete!\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample Results:\")\n",
    "print(results_df.head())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNQaOiyIeyyTnGN62jU2aFF",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1affa6f452934e78987d0266554b952c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20a427473aa54832b14f1d964f2d7deb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30c3d1a447524554a66a3a2126617242": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae8f62824c3d4f8eac635485b3532284",
       "IPY_MODEL_b3ac3b381a7e4144bc778d05269c9284",
       "IPY_MODEL_a8cae0c45b1249028946d8028160eae5"
      ],
      "layout": "IPY_MODEL_20a427473aa54832b14f1d964f2d7deb"
     }
    },
    "56c17f16fbf94c0d845d931285c4fd81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "776b9735bfb141628f50295c5970d13b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86a5def4b609496a8e57518674d0181b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fde096508bb4e2cb0a257d39b6d5574": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8cae0c45b1249028946d8028160eae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e85b63ef607249faa67da2e2691455a8",
      "placeholder": "​",
      "style": "IPY_MODEL_776b9735bfb141628f50295c5970d13b",
      "value": " 4/4 [00:06&lt;00:00,  1.40s/it]"
     }
    },
    "ae8f62824c3d4f8eac635485b3532284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1affa6f452934e78987d0266554b952c",
      "placeholder": "​",
      "style": "IPY_MODEL_86a5def4b609496a8e57518674d0181b",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b3ac3b381a7e4144bc778d05269c9284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fde096508bb4e2cb0a257d39b6d5574",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_56c17f16fbf94c0d845d931285c4fd81",
      "value": 4
     }
    },
    "e85b63ef607249faa67da2e2691455a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
